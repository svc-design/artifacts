global:
  imageRegistry: ghcr.io/your-org
  namespace: default
  ingress:
    enabled: true
    className: nginx
    host: api.svc.plus
    tls: true
    tlsSecretName: model-serving-tls

models:
  - name: llama3-8b-vllm
    engine: vllm
    image: "model-serving/vllm"
    tag: "cuda12"
    replicas: 1
    path: v1/llama3
    env:
      - name: MODEL_PATH
        value: "meta-llama/Meta-Llama-3-8B-Instruct"
      - name: VLLM_ARGS
        value: "--max-model-len 8192 --gpu-memory-utilization 0.9"
    resources:
      limits:
        nvidia.com/gpu: 1
    nodeSelector: {}
    tolerations: []

  - name: qwen2-7b-sglang
    engine: sglang
    image: "model-serving/sglang"
    tag: "cuda12"
    replicas: 1
    path: v1/qwen2
    env:
      - name: SGLANG_MODEL
        value: "Qwen/Qwen2-7B-Instruct"
      - name: SGLANG_ARGS
        value: "--tp 1 --context-length 8192"
    resources:
      limits:
        nvidia.com/gpu: 1

  - name: phi3-ollama
    engine: ollama
    image: "model-serving/ollama"
    tag: "latest"
    replicas: 1
    path: v1/phi3
    env:
      - name: OLLAMA_MODEL
        value: "phi3:latest"
    resources:
      limits:
        nvidia.com/gpu: 1

service:
  type: ClusterIP
  port: 80

imagePullSecrets: []
